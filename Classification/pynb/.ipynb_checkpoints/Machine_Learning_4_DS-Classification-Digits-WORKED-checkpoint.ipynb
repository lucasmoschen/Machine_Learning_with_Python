{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<span style=\"font-family:Georgia; font-size:2.0em;\"> PROGRAMA DE VERÃO FGV EMAp 2019</span> <br><br>\n",
    "<span style=\"font-family:Georgia; font-size:1.5em;\"> INTRODUCTION TO MACHINE LEARNING WITH PYTHON</span> <br><br>\n",
    "<span style=\"font-family:Georgia; font-size:1.5em;\"> Luis Gustavo Nonato </span> <br>\n",
    "<span style=\"font-family:Georgia; font-size:1.0em;\">ICMC-USP, Brazil </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X_data = digits.data\n",
    "Y_classes = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# K-fold CV\n",
    "###########\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_classes, train_size=0.7,test_size = 0.3, random_state=3)\n",
    "test_size = Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Naive Bayes----------\n",
      "60 misclassified data out of 540 ( 11.11111111111111 %)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Naive Bayes\n",
    "###########\n",
    "nb = naive_bayes.MultinomialNB().fit(X_train,Y_train)\n",
    "# predicting test data\n",
    "ypred_nb = nb.predict(X_test) \n",
    "# computing error\n",
    "error_nb = sum((ypred_nb[i] != Y_test[i]) for i in range(0,test_size))  ##soma valores 1 para cada erro. Se acertar, é 0. \n",
    "print(\"----------Naive Bayes----------\")\n",
    "print(error_nb, \"misclassified data out of\", test_size, \"(\",100*(error_nb/test_size),\"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------SVM Linear----------\n",
      "number of support vectors 379\n",
      "19 misclassified data out of 540 ( 3.5185185185185186 %)\n",
      "----------SVM RBF----------\n",
      "number of support vectors 639\n",
      "4 misclassified data out of 540 ( 0.7407407407407407 %)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "## # SVM linear\n",
    "###########\n",
    "svm_linear = svm.SVC(kernel='linear').fit(X_train,Y_train)\n",
    "\n",
    "# predicting test data\n",
    "ypred_svm_linear = svm_linear.predict(X_test)\n",
    "\n",
    "# computing error\n",
    "error_svm_linear = sum((ypred_svm_linear[i] != Y_test[i]) for i in range(0,test_size))\n",
    "print(\"----------SVM Linear----------\")\n",
    "print(\"number of support vectors\",len(svm_linear.support_))\n",
    "print(error_svm_linear, \"misclassified data out of\", test_size, \"(\",100*(error_svm_linear/test_size),\"%)\")\n",
    "\n",
    "###########\n",
    "# SVM kernel RBF\n",
    "###########\n",
    "#class_weight={2:10} gives a larger importance to class 2\n",
    "#svm_rbf = svm.SVC(kernel='rbf')\n",
    "svm_rbf = svm.SVC(kernel='rbf', gamma=1e-3)\n",
    "svm_rbf.fit(X_train,Y_train.ravel())\n",
    " \n",
    "# predicting test data\n",
    "ypred_svm_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# computing error\n",
    "error_svm_rbf = sum((ypred_svm_rbf[i] != Y_test[i]) for i in range(0,test_size))\n",
    "print(\"----------SVM RBF----------\")\n",
    "print(\"number of support vectors\",len(svm_rbf.support_))\n",
    "print(error_svm_rbf, \"misclassified data out of\", test_size, \"(\",100*error_svm_rbf/test_size,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"yeast3.dat\",header=None)\n",
    "\n",
    "X_data = df.iloc[:,0:8].values\n",
    "Y_data = np.asarray([0 if df.iloc[i,-1] == ' negative' else 1 for i in range(df.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------SVM RBF----------\n",
      "25 misclassified data out of 446 ( 5.605381165919282 %)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, train_size=0.7,test_size = 0.3, random_state=3)\n",
    "test_size = Y_test.shape[0]\n",
    "\n",
    "svm_rbf = svm.SVC(kernel='rbf', gamma=8)\n",
    "svm_rbf.fit(X_train,Y_train)\n",
    "ypred_svm_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# computing error\n",
    "error_svm_rbf = sum((ypred_svm_rbf[i] != Y_test[i]) for i in range(0,test_size))\n",
    "print(\"----------SVM RBF----------\")\n",
    "print(error_svm_rbf, \"misclassified data out of\", test_size, \"(\",100*error_svm_rbf/test_size,\"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------SVM RBF----------\n",
      "57 misclassified data out of 446 ( 12.780269058295964 %)\n"
     ]
    }
   ],
   "source": [
    "ypred = np.zeros((test_size,))\n",
    "\n",
    "error_0 = sum((ypred[i] != Y_test[i]) for i in range(0,test_size))\n",
    "print(\"----------SVM RBF----------\")\n",
    "print(error_0, \"misclassified data out of\", test_size, \"(\",100*error_0/test_size,\"%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Weiss, Gary M., Kate McCarthy, and Bibi Zabar. \"Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs?.\" DMIN 7:35-41, 2007.](https://scholar.google.com/scholar?cluster=10779872536070567255&hl=en&as_sdt=0,22) \n",
    "\n",
    "[https://pdfs.semanticscholar.org/9908/404807bf6b63e05e5345f02bcb23cc739ebd.pdf](https://pdfs.semanticscholar.org/9908/404807bf6b63e05e5345f02bcb23cc739ebd.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
